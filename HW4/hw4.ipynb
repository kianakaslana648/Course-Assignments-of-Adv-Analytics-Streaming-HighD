{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Graphical LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Matrix Completion for Movielens\n",
    "\n",
    "### Part A: \n",
    "\n",
    "Download the Movielens 100k dataset:\n",
    "\n",
    "https://grouplens.org/datasets/movielens/100k/\n",
    "\n",
    "This dataset consists of 100k entries of a 1000 by 1700 matrix, so there are many missing entries. The goal is to complete the matrix and recommend movies to a user if the completed entry has a high rating. \n",
    "\n",
    "After reading this data into python, naively complete the ratings for each movie by first computing the movie's median rating over known entries, and complete the missing ratings for the movie with this median rating.\n",
    "\n",
    "Thinking of each user as random vector over the movie ratings, use *sklearn.covariance.GraphicalLassoCV* to fit a precision matrix to the random variables of movie ratings (remember to subtract the means before computing the empirical covariance matrix). \n",
    "\n",
    "Use this precision matrix and the conditional mean formula to complete the matrix. Plot the completed matrix as a 1000 by 1700 pixel image using *matplotlib.pyplot*.\n",
    "\n",
    "### Part B: \n",
    "\n",
    "Perform 25 bootstrap runs to estimate the generalization error of this procedure. For each of the 25 runs, randomly choose a training set of 70k known entries to complete the matrix as in Part A, and then compute the $\\ell_1$ empirical risk on the remaining 30k known entries of test data. Provide a box and whisker plot for these empirical risks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rd\n",
    "import random\n",
    "import plotly.express as px\n",
    "import numpy.linalg as la\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('u.data',sep='\\t', names=['user','item','rating','timestamp'])\n",
    "df=df.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "df_total=csr_matrix((np.array(df['rating']),(np.array(df['user'])-1,np.array(df['item'])-1)),shape=(943,1682)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=np.array(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1682):\n",
    "    tempCol = df_total[:,i]\n",
    "    reCol = tempCol[tempCol!=0]\n",
    "    tempCol[tempCol==0]=np.median(reCol)\n",
    "    df_total[:,i]=tempCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_copy = np.array(df_total).astype(np.float)\n",
    "means=df_copy.mean(axis=0)\n",
    "stds=df_copy.std(axis=0)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df_copy = scaler.fit_transform(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_col = (df_copy!=0).sum(axis=0) != 0\n",
    "one_col = (df_copy!=0).sum(axis=0) == 0\n",
    "df_pro = df_copy[:,rem_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphaRange = (10.0 ** np.arange(0,3)).tolist()\n",
    "model=GraphicalLassoCV(alphas=alphaRange, mode='cd')\n",
    "model.fit(df_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov=model.covariance_\n",
    "K = model.precision_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = np.array(df_raw)\n",
    "indices = np.array([i for i in range(1682)])\n",
    "for i in range(943):\n",
    "    cur_ind=df_raw[i,rem_col]!=0\n",
    "    pre_ind=df_raw[i,rem_col]==0\n",
    "    temp = (means[rem_col])[pre_ind]-cov[np.ix_(pre_ind,pre_ind)]@K[np.ix_(pre_ind,cur_ind)]\\\n",
    "    @(df_pro[np.ix_([i],cur_ind)].flatten())*(stds[rem_col])[pre_ind]\n",
    "    df_new[i,indices[rem_col][pre_ind]]=np.round(temp)\n",
    "\n",
    "for i in indices[one_col]:\n",
    "    df_new[:,i]=int(round(means[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tol_ind = np.array([i for i in range(100000)])\n",
    "errors = np.array([0]*25)\n",
    "\n",
    "\n",
    "for j in range(25):\n",
    "    print('step'+str(j+1))\n",
    "    train_ind=random.sample(range(100000),70000)\n",
    "    test_ind= np.setdiff1d(tol_ind,train_ind)\n",
    "    train_cur=df.loc[index_cur,:]\n",
    "    test_cur=df.loc[test_ind,:]\n",
    "\n",
    "\n",
    "    temp_df_total=csr_matrix((np.array(train_cur['rating']),\\\n",
    "                              (np.array(train_cur['user'])-1,np.array(train_cur['item'])-1)),shape=(943,1682)).toarray()\n",
    "    temp_df_raw =np.array(temp_df_total)\n",
    "\n",
    "    for i in range(1682):\n",
    "        tempCol = temp_df_total[:,i]\n",
    "        reCol = tempCol[tempCol!=0]\n",
    "        if len(reCol)>0:\n",
    "            tempCol[tempCol==0]=np.median(reCol)\n",
    "        else:\n",
    "            print('No data in Column'+str(i+1))\n",
    "            tempCol=np.array([3]*943)\n",
    "        temp_df_total[:,i]=tempCol\n",
    "\n",
    "    temp_df_copy = np.array(temp_df_total).astype(np.float)\n",
    "    temp_means=temp_df_copy.mean(axis=0)\n",
    "    temp_stds=temp_df_copy.std(axis=0)\n",
    "    temp_scaler = preprocessing.StandardScaler()\n",
    "    temp_df_copy = temp_scaler.fit_transform(temp_df_copy)\n",
    "\n",
    "    temp_rem_col = (temp_df_copy!=0).sum(axis=0) != 0\n",
    "    temp_one_col = (temp_df_copy!=0).sum(axis=0) == 0\n",
    "    temp_df_pro = temp_df_copy[:,temp_rem_col]\n",
    "\n",
    "    temp_alphaRange = (10.0 ** np.arange(0,3)).tolist()\n",
    "    temp_model=GraphicalLassoCV(alphas=temp_alphaRange, mode='cd')\n",
    "    temp_model.fit(temp_df_pro)\n",
    "\n",
    "    temp_cov = temp_model.covariance_\n",
    "    temp_K = temp_model.precision_\n",
    "\n",
    "    temp_df_new = np.array(temp_df_raw)\n",
    "    temp_indices = np.array([i for i in range(1682)])\n",
    "    for i in range(943):\n",
    "        temp_cur_ind=temp_df_raw[i,temp_rem_col]!=0\n",
    "        temp_pre_ind=temp_df_raw[i,temp_rem_col]==0\n",
    "        temp = (temp_means[temp_rem_col])[temp_pre_ind]-temp_cov[np.ix_(temp_pre_ind,temp_pre_ind)]\\\n",
    "        @ temp_K[np.ix_(temp_pre_ind,temp_cur_ind)]\\\n",
    "        @(temp_df_pro[np.ix_([i],temp_cur_ind)].flatten())*(temp_stds[temp_rem_col])[temp_pre_ind]\n",
    "        temp_df_new[i,temp_indices[temp_rem_col][temp_pre_ind]]=np.round(temp)\n",
    "\n",
    "    for i in temp_indices[temp_one_col]:\n",
    "        temp_df_new[:,i]=int(round(temp_means[i]))\n",
    "\n",
    "    error = 0\n",
    "    for i in range(test_cur.shape[0]):\n",
    "        error += np.abs(temp_df_new[test_cur.iloc[i,0]-1,test_cur.iloc[i,1]-1]-test_cur.iloc[i,2])\n",
    "    errors[j]=error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "px.box(y=errors,title='L1 Empirical Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Graphical LASSO fit for Riboflavin Data\n",
    "\n",
    "Append the target ($y$) variable to the Riboflavin feature dataset ($X$), remove the empirical means from the columns of $(y\\:X)$, and then use *sklearn.covariance.GraphicalLassoCV* to fit a precision matrix for this appended dataset. Note that this method automatically chooses the $\\lambda$ parameter using cross validation.\n",
    "\n",
    "Under the assumption of normality, use precision matrix on the dataset plus target to approximate the target as a sparse linear combination of the the predictors (as discussed in Lecture 08). Sort all of the target values $y$ in increasing order, and plot these against the predicted values to visually assess the regression. Describe your results and compare with the LASSO solution from HW3 Problem 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rx=pd.read_csv('riboflavin/riboflavinx.csv')\n",
    "df_ry=pd.read_csv('riboflavin/riboflaviny.csv')\n",
    "rx=np.array(df_rx.iloc[:,1:4089])\n",
    "ry=np.array(df_ry.iloc[:,1])\n",
    "ry = np.expand_dims(ry, axis=0)\n",
    "selected = random.sample([i for i in range(4089)],200)\n",
    "rx = rx[:,selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribof=np.concatenate((rx,ry.T),axis=1)\n",
    "means = ribof.mean(axis=0)\n",
    "ribof-=means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=GraphicalLassoCV(mode='cd',n_jobs=-1)\n",
    "model2.fit(ribof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_ribof = model2.covariance_\n",
    "K_ribof = model2.precision_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([0]*71)\n",
    "for i in range(71):\n",
    "    y_pred[i] = means[200]-cov_ribof[200,200]*K_ribof[200,0:200]@ribof[i,0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=y_pred, y=ribof[:,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5*la.norm(y_pred-ribof[:,200])**2/71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thres(x,lamb):\n",
    "    if(x>lamb):\n",
    "        return(x-lamb)\n",
    "    if(x<-lamb):\n",
    "        return(x+lamb)\n",
    "    return 0\n",
    "def coord_des_lasso(y, X, beta0, lamb, error, maxstep):\n",
    "    n=X.shape[0]\n",
    "    p=X.shape[1]\n",
    "    beta = beta0\n",
    "    r_prev = 0.5*la.norm(y-X@beta)**2+lamb*la.norm(beta,ord=1)\n",
    "    step = 0\n",
    "    while(step < maxstep):\n",
    "        #print(\"#######\")\n",
    "        #print(\"Step\"+str(step))\n",
    "        for i in range(p):\n",
    "            beta_temp=np.array(beta)\n",
    "            beta_temp[i]=0\n",
    "            y_temp = y-X@beta_temp\n",
    "            beta[i]=soft_thres(np.dot(y_temp,X[:,i]),lamb)/(la.norm(X[:,i])**2)\n",
    "        r_cur = 0.5*la.norm(y-X@beta)**2+lamb*la.norm(beta,ord=1)\n",
    "        if(np.abs(r_cur-r_prev)<error or step==maxstep-1):\n",
    "            err_final=np.abs(r_cur-r_prev)\n",
    "            break\n",
    "        else:\n",
    "            r_prev=r_cur\n",
    "            step=step+1\n",
    "    return beta, step, err_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prederr_record=np.zeros((5,25))\n",
    "ry=ry.flatten()\n",
    "ind = [-2,-1,0,1,2]\n",
    "for i in range(5):\n",
    "    print('#####')\n",
    "    print(\"lambda_\"+str(i+1))\n",
    "    for j in range(25):\n",
    "        print(\"  step_\"+str(j+1))\n",
    "        train_index=rd.choice(np.arange(71),50,replace=False)\n",
    "        test_index=np.setdiff1d(np.arange(71),train_index)\n",
    "\n",
    "        train_X = rx[train_index,:]\n",
    "        train_y = ry[train_index]\n",
    "        test_X = rx[test_index,:]\n",
    "        test_y = ry[test_index]\n",
    "\n",
    "        beta0=np.zeros(train_X.shape[1])\n",
    "        lamb = np.log(train_X.shape[1])*(10**ind[i])\n",
    "        beta, step, err = coord_des_lasso(train_y,train_X,beta0,lamb,0.1,100)\n",
    "\n",
    "        pred_err = 0.5*la.norm(test_y-test_X@beta)**2\n",
    "        prederr_record[i,j]=pred_err\n",
    "prederr_record /= 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(prederr_record.T, columns=['k=-2','k=-1','k=0','k=1','k=2'])\n",
    "df_tidy = pd.melt(df_temp)\n",
    "\n",
    "px.box(df_tidy, x='variable', y='value', color='variable',title='Predictive Errors Using Different Lambdas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "While we can achieve good results through Graphical Lasso, it takes a lot of time. Thus, we have to select a subset of all the features and then do the graphical lasso. But the loss of information leads to a bad performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Graphical LASSO and LDA for Prostate Dataset\n",
    "\n",
    "\n",
    "In this problem, you will modify the code from HW3 Problem 3 to train a regularized Linear Discriminant Analysis (LDA) classifier. Split the data as in HW3 Problem 3, and use the training data to fit conditional means and conditional precision matrices to the two classes. That is, let $C_0=\\{i:y_i=0\\}$ and $C_1=\\{i:y_i=1\\}$\n",
    "$$\n",
    "\\hat\\mu_0 = \\frac{1}{\\text{card} C_0} \\sum_{i\\in C_0} x_i\\text{ and }\\hat\\mu_1 = \\frac{1}{\\text{card} C_1} \\sum_{i\\in C_1} x_i\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\hat\\Sigma_0 = \\frac{1}{\\text{card} C_0} \\sum_{i\\in C_0} (x_i-\\hat\\mu_i)^T(x_i-\\hat\\mu_i)\\text{ and }\\hat\\Sigma_1 = \\frac{1}{\\text{card} C_1} \\sum_{i\\in C_1} (x_i-\\hat\\mu_i)^T(x_i-\\hat\\mu_i).\n",
    "$$\n",
    "Use *sklearn.covariance.GraphicalLassoCV* to fit $K_0$ and $K_1$ using $\\Sigma_0$ and $\\Sigma_1$.\n",
    "\n",
    "We define the Linear Discriminant as\n",
    "$$\n",
    "f(x) = (x-\\hat\\mu_0)^T K_0(x-\\hat\\mu_0) - \\ln\\det K_0 - (x-\\hat\\mu_1)^T K_1(x-\\hat\\mu_1) + \\ln\\det K_1\n",
    "$$\n",
    "which induces the prediction\n",
    "$$\n",
    "\\text{pred}_\\eta(x) = \\left\\{\\begin{array}{cl} 0 &f(x)\\leq \\eta\\\\ 1 & f(x)>\\eta\\end{array}\\right.\n",
    "$$\n",
    "Here, $\\eta$ is chosen to manage the tradeoff between Type I and Type II errors. \n",
    "\n",
    "### Part A: \n",
    "\n",
    "In LDA, it is often assumed that $K_0=K_1$ so that the linear discriminant function simplifies. This is called *heteroscedasticity*. Does this assumption appear to be true for the prostate data set?\n",
    "\n",
    "### Part B:\n",
    "\n",
    "Run your code one time, and display the ROC (receiver operating characteristic) curve for the LDA classifier over the test dataset. A good measure of the usefulness of a classifier is the area under the ROC curve, and a classifier is good if the area under the curve is very close to $1$. It the LDA classifier \"good\" according to this measure?\n",
    "\n",
    "### Part C: \n",
    "\n",
    "The logistic regression classifier from HW3 Problem 3 can also be modified to admit an ROC curve. After fitting $p(y=0|x)$ and $p(y=1|x)$, one can predict $y=1$ for $x$ if\n",
    "\n",
    "$$\n",
    "\\ln p(y=1|x) - \\ln p(y=0|x) > \\eta.\n",
    "$$\n",
    "\n",
    "For a single run of training/testing, simultaneously display the ROC curve for this LDA classifier and the $\\ell_1$ regularized logistic regression classifier trained on the same data. Discuss your observations.\n",
    "\n",
    "\n",
    "### Part D: \n",
    "\n",
    "Discuss reasons why you would prefer the regularized logisitic regression classifier over this regularized LDA classifier and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX = pd.read_csv('prostate/prostate.csv')\n",
    "pY = pX.iloc[:,-1]\n",
    "pX = pX.iloc[:,1:6034]\n",
    "selected = random.sample([i for i in range(4089)],200)\n",
    "pX = pX.iloc[:,selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array([i for i in range(102)])\n",
    "pos_ind = indices[pY==1]\n",
    "neg_ind = indices[pY==0]\n",
    "\n",
    "pos_X = pX.iloc[pos_ind,:]\n",
    "neg_X = pX.iloc[neg_ind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_means = pos_X.mean(axis=0)\n",
    "neg_means = neg_X.mean(axis=0)\n",
    "pos_X -= pos_means\n",
    "neg_X -= neg_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=GraphicalLassoCV(n_jobs=-1)\n",
    "model3.fit(pos_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4=GraphicalLassoCV(n_jobs=-1)\n",
    "model4.fit(neg_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = model3.precision_\n",
    "K2 = model4.precision_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(K1!=K2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "This is not true for this random subset of the Prostate dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(x, yeta):\n",
    "    judge = (x-pos_means)@K1@(x-pos_means).T-np.log(la.det(K1))-(x-neg_means)@K2@(x-neg_means).T+np.log(la.det(K2))-yeta\n",
    "    if judge > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def ROC_compute(yeta):\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for i in range(71):\n",
    "        x = pX.iloc[i,:]\n",
    "        temp_pred=LDA(x,yeta)\n",
    "        if temp_pred==1:\n",
    "            if pY[i]==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if pY[i]==1:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "    return TP/(TP+FN), FP/(FP+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crit(x, yeta):\n",
    "    return (x-pos_means)@K1@(x-pos_means).T-np.log(la.det(K1))-(x-neg_means)@K2@(x-neg_means).T+np.log(la.det(K2))-yeta\n",
    "#pX.apply(crit,axis=1,args=(0))\n",
    "crit_result=pX.apply(crit,axis=1,args=(0,))\n",
    "min_yeta= np.min(crit_result)-0.1\n",
    "max_yeta= np.max(crit_result)+0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_yeta = 40\n",
    "yetas = np.arange(min_yeta, max_yeta, (max_yeta-min_yeta)/num_of_yeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR = np.zeros(num_of_yeta)\n",
    "FPR = np.zeros(num_of_yeta)\n",
    "for i in range(num_of_yeta):\n",
    "    yeta = yetas[i]\n",
    "    TPR[i], FPR[i] = ROC_compute(yeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=TPR,y=FPR,title='ROC Curve of Subset Graphical Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "The 'subset' graphical lasso seems not to be good due to the loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = rd.choice(np.arange(101),80,replace=False)\n",
    "test_index = np.setdiff1d(np.arange(101),train_index)\n",
    "pX_train = pX.iloc[train_index,:]\n",
    "pY_train = pY[train_index]\n",
    "pX_test = pX.iloc[test_index,:]\n",
    "pY_test = np.array(pY[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = LogisticRegression(penalty='l1', solver='saga', max_iter=1000, random_state=37).fit(pX_train, pY_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = model5.predict_log_proba(pX_test)\n",
    "log_res = (log_prob[:,1]-log_prob[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_yeta2 = np.min(log_res)\n",
    "max_yeta2 = np.max(log_res)\n",
    "\n",
    "num_of_yeta = 40\n",
    "yetas2 = np.arange(min_yeta2, max_yeta2, (max_yeta2-min_yeta2)/num_of_yeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_compute_2(yeta):\n",
    "    y_pred = np.where(log_res>yeta,1,0)\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]==1:\n",
    "            if pY_test[i]==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if pY_test[i]==1:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "    return TP/(TP+FN), FP/(FP+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR2 = np.zeros(num_of_yeta)\n",
    "FPR2 = np.zeros(num_of_yeta)\n",
    "for i in range(num_of_yeta):\n",
    "    yeta2 = yetas2[i]\n",
    "    TPR2[i], FPR2[i] = ROC_compute_2(yeta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=TPR2,y=FPR2,title='ROC Curve of L1 Penalized Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "The performance of Logistic Regression in this case in  metric of ROC is rather bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "As we can see, if we judge the performance in the metric of ROC curve, the regularized LDA is better than the regularized logisitic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: S&P 500 Log Returns Data\n",
    "\n",
    "\n",
    "Consider the following code for the the conditional dependence structure on the log-returns of 404 symbols in the S&P 500 data across two successive days.\n",
    "\n",
    "### Part A: Describe the plots produced by this code. First, explain what the plots are displaying, and then interpret what the plots tell you about the the data.\n",
    "\n",
    "### Part B: Modify this code to produce a sparse precision matrix for the log returns of the symbols based on the first 150 days of information only, and use the structure of this precision matrix to determine small subsets of predictors from the first day and responses from the second day that exhibit a dependence structure. Replace each response variable from this small list with an indicator variable that is 1 if the variable is positive and 0 if the variable is negative, and perform logistic regression of the small subset of predictors on this indicator. Plot the ROC curves for each of these logisitic regression models on the next 100 days of data, and discuss your results.\n",
    "\n",
    "### Part C: For each response symbol found in Part B, perform $\\ell^1$-penalized logisitic regression using all 404 symbols log-returns on the previous day. Use the function *sklearn.linear_model.LogisiticRegression* with the argument *penalty='l1'* to train on the first 150 days of data, and then provide ROC curves for these models on the next 100 days of data. Compare these results with the results from Part B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"./finance/data.csv\")\n",
    "\n",
    "# Symbols with nan data\n",
    "nanix = [7, 52, 56, 65, 70, 71, 72, 75, 94, 99, 122, 125, 138, 170, 174, 177, 193, 203, 226, 260, 282, 308, 309, 326, 335, 352, 365, 389, 404]\n",
    "\n",
    "data = np.zeros((433-len(nanix),251))\n",
    "k = 0\n",
    "\n",
    "for i in range(433):\n",
    "    q = np.diff(np.log(np.array(df['Open.' + str(i+1)][2:].astype(float))))\n",
    "    if np.sum(np.isnan(q)) == 0:\n",
    "        data[k,:]=np.diff(np.log(np.array(df['Open.' + str(i+1)][2:].astype(float))))\n",
    "        k=k+1\n",
    "        \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "x = scaler.transform(data)\n",
    "\n",
    "# Data from successive trading days\n",
    "seq_data = np.concatenate((x[:,0:250], x[:,1:251]), axis=0)\n",
    "   \n",
    "sig = np.ma.cov(seq_data) #Removes NANs from covariance computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import graphical_lasso\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "\n",
    "def display_graph(A, node_color, node_text):\n",
    "    G = nx.from_numpy_matrix(A)\n",
    "    X = nx.spring_layout(G)\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0 = X[edge[0]]\n",
    "        x1 = X[edge[1]]\n",
    "        edge_x.append(x0[0])\n",
    "        edge_x.append(x1[0])\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(x0[1])\n",
    "        edge_y.append(x1[1])\n",
    "        edge_y.append(None)\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.5, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines')\n",
    "\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    for node in G.nodes():\n",
    "        x = X[node]\n",
    "        node_x.append(x[0])\n",
    "        node_y.append(x[1])\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            # colorscale options\n",
    "            #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |\n",
    "            #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |\n",
    "            #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |\n",
    "            colorscale='YlGnBu',\n",
    "            reversescale=True,\n",
    "            color=[],\n",
    "            size=10,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                #title='Node Connections',\n",
    "                xanchor='left',\n",
    "                titleside='right'\n",
    "            ),\n",
    "            line_width=2))\n",
    "\n",
    "    node_trace.marker.color = node_color\n",
    "    node_trace.text = node_text\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                 layout=go.Layout(\n",
    "                    title='2D Network Graph',\n",
    "                    titlefont_size=16,\n",
    "                    showlegend=False,\n",
    "                    hovermode='closest',\n",
    "                    margin=dict(b=20,l=5,r=5,t=40),\n",
    "                    annotations=[ dict(\n",
    "                        text=\"\", #text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "                        showarrow=False,\n",
    "                        xref=\"paper\", yref=\"paper\",\n",
    "                        x=0.005, y=-0.002 ) ],\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                    )\n",
    "    fig.show()\n",
    "    \n",
    "def display_graph_3D(A, node_color, node_text):\n",
    "    G = nx.from_numpy_matrix(A)\n",
    "    X = nx.spring_layout(G, dim=3)\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_z = []\n",
    "    for edge in G.edges():\n",
    "        x0 = X[edge[0]]\n",
    "        x1 = X[edge[1]]\n",
    "        edge_x.append(x0[0])\n",
    "        edge_x.append(x1[0])\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(x0[1])\n",
    "        edge_y.append(x1[1])\n",
    "        edge_y.append(None)\n",
    "        edge_z.append(x0[2])\n",
    "        edge_z.append(x1[2])\n",
    "        edge_z.append(None)\n",
    "\n",
    "    edge_trace = go.Scatter3d(\n",
    "        x=edge_x, y=edge_y, z=edge_z,\n",
    "        line=dict(width=0.5, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines')\n",
    "\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_z = []\n",
    "    for node in G.nodes():\n",
    "        x = X[node]\n",
    "        node_x.append(x[0])\n",
    "        node_y.append(x[1])\n",
    "        node_z.append(x[2])\n",
    "\n",
    "    node_trace = go.Scatter3d(\n",
    "        x=node_x, y=node_y, z=node_z,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            # colorscale options\n",
    "            #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |\n",
    "            #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |\n",
    "            #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |\n",
    "            colorscale='YlGnBu',\n",
    "            reversescale=True,\n",
    "            color=[],\n",
    "            size=5,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                #title='Node Connections',\n",
    "                xanchor='left',\n",
    "                titleside='right'\n",
    "            ),\n",
    "            line_width=2))\n",
    "\n",
    "    node_trace.marker.color = node_color\n",
    "    node_trace.text = node_text\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                 layout=go.Layout(\n",
    "                    title='3D Network Graph',\n",
    "                    titlefont_size=16,\n",
    "                    showlegend=False,\n",
    "                    hovermode='closest',\n",
    "                    margin=dict(b=20,l=5,r=5,t=40),\n",
    "                    annotations=[ dict(\n",
    "                        text=\"\", #text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "                        showarrow=False,\n",
    "                        xref=\"paper\", yref=\"paper\",\n",
    "                        x=0.005, y=-0.002 ) ],\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                    )\n",
    "    fig.show()\n",
    "    \n",
    "A = np.matrix([[0, 1, 0, 1],\n",
    "               [1, 0, 1, 1],\n",
    "               [0, 1, 0, 1],\n",
    "               [1, 0, 1, 0]])\n",
    "\n",
    "node_color = [2, 3, 2, 3]\n",
    "node_text = ['1', '2', '3', '4']\n",
    "\n",
    "\n",
    "display_graph_3D(A, node_color, node_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_cov = sig.data\n",
    "cov, pre = graphical_lasso(emp_cov,0.6,verbose=True)\n",
    "\n",
    "tau = np.percentile(np.abs(np.ndarray.flatten(pre)),99)\n",
    "fig = go.Figure(data=go.Heatmap(z=pre, x=[], y=[], zmin=-tau, zmax=tau))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.abs(pre - np.diag(np.diag(pre)))\n",
    "#tau = np.percentile(np.abs(A),99)\n",
    "#A = (A > tau)  \n",
    "\n",
    "A = (A > 0)\n",
    "\n",
    "fig = go.Figure(data=go.Histogram(x=np.matrix.flatten(np.abs(pre[A]))))\n",
    "fig.show()\n",
    "\n",
    "A = A.astype(int)\n",
    "fig = go.Figure(data=go.Heatmap(z=A, x=[], y=[]))\n",
    "fig.show()\n",
    "\n",
    "node_color = np.diag(pre)\n",
    "node_text = [] * 404\n",
    "\n",
    "display_graph(A, node_color, node_text)\n",
    "\n",
    "display_graph_3D(A, node_color, node_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "**(1)** The heatmap of the precision matrix is shown. We can see the diagonal elements are the significant ones while other elements are minor. That indicates small correlation between variables.\n",
    "\n",
    "**(2)** The histogram of non-zero elements that are not in the diagonal line is shown. We can see most of them are relatively small.\n",
    "\n",
    "**(3)** The heatmap of all non-zero elements that are not in the diagonal line is shown. By converting all those elements into values of 1, we can clearly catch those non-zero elements.\n",
    "\n",
    "**(4)** The 2D network graph of all variables in the design matrix is shown. We can see that most nodes are not correlated.\n",
    "\n",
    "**(5)** The 3D network graph of all variables in the design matrix is shown. Also we can see that most nodes are not correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_data = seq_data[:,0:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_des = np.ma.cov(des_data)\n",
    "emp_cov = sig_des.data\n",
    "cov, pre = graphical_lasso(emp_cov,0.6,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.abs(pre - np.diag(np.diag(pre)))\n",
    "A = (A > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = np.arange(404)[(A[404:808,0:404].sum(axis=1)!=0).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_compute_3(yeta):\n",
    "    y_pred = np.where(temp_log_res>yeta,1,0)\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]==1:\n",
    "            if temp_y[i]==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if temp_y[i]==1:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "    return TP/(TP+FN), FP/(FP+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(response_list)):\n",
    "    index = response_list[i]\n",
    "    predictor_list = np.arange(404)[(A[404+index,:]==1)[0:404]]\n",
    "    temp_X = des_data[predictor_list,:].T\n",
    "    temp_y = np.where(des_data[404+index,:]>0,1,0)\n",
    "    temp_model = LogisticRegression(penalty='l1', solver='saga', max_iter=1000, random_state=37).fit(temp_X, temp_y)\n",
    "    \n",
    "    temp_X_test = seq_data[predictor_list,150:250].T\n",
    "    temp_y_test = np.where(seq_data[404+index,150:250]>0,1,0)\n",
    "\n",
    "    temp_log_prob = temp_model.predict_log_proba(temp_X_test)\n",
    "    temp_log_res = (temp_log_prob[:,1]-temp_log_prob[:,0])\n",
    "\n",
    "    temp_min_yeta2 = np.min(temp_log_res)-0.1\n",
    "    temp_max_yeta2 = np.max(temp_log_res)+0.1\n",
    "\n",
    "    num_of_yeta = 40\n",
    "    temp_yetas2 = np.arange(temp_min_yeta2, temp_max_yeta2, (temp_max_yeta2-temp_min_yeta2)/num_of_yeta)\n",
    "   \n",
    "    temp_TPR2 = np.zeros(num_of_yeta)\n",
    "    temp_FPR2 = np.zeros(num_of_yeta)\n",
    "    for j in range(num_of_yeta):\n",
    "        yeta2 = temp_yetas2[j]\n",
    "        temp_TPR2[j], temp_FPR2[j] = ROC_compute_3(yeta2)\n",
    "\n",
    "    px.line(x=temp_TPR2,y=temp_FPR2,title=str(i)+'.'+' ROC Curve of Variable '+str(index)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "For most symbols, we have an ROC curve close to the diagonal line, which indicates we can gain a model with a pair of TPR and FPR approximately being (0.5, 0.5) (Or a little bit better than that). So we are not gaining a model with high accuracy and precision, but at least we are not gaining a model too bad (They're not below the diagonal line too far). So that's the common performance in the time-series analysis. If we use the similar method with the previous log return variables more than one day, the performance can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(response_list)):\n",
    "    temp_X = des_data[0:404,:].T\n",
    "    temp_y = np.where(des_data[404+index,:]>0,1,0)\n",
    "    temp_model = LogisticRegression(penalty='l1', solver='saga', max_iter=1000, random_state=37).fit(temp_X, temp_y)\n",
    "    \n",
    "    temp_X_test = seq_data[0:404,150:250].T\n",
    "    temp_y_test = np.where(seq_data[404+index,150:250]>0,1,0)\n",
    "\n",
    "    temp_log_prob = temp_model.predict_log_proba(temp_X_test)\n",
    "    temp_log_res = (temp_log_prob[:,1]-temp_log_prob[:,0])\n",
    "\n",
    "    temp_min_yeta2 = np.min(temp_log_res)-0.1\n",
    "    temp_max_yeta2 = np.max(temp_log_res)+0.1\n",
    "\n",
    "    num_of_yeta = 40\n",
    "    temp_yetas2 = np.arange(temp_min_yeta2, temp_max_yeta2, (temp_max_yeta2-temp_min_yeta2)/num_of_yeta)\n",
    "   \n",
    "    temp_TPR2 = np.zeros(num_of_yeta)\n",
    "    temp_FPR2 = np.zeros(num_of_yeta)\n",
    "    for j in range(num_of_yeta):\n",
    "        yeta2 = temp_yetas2[j]\n",
    "        temp_TPR2[j], temp_FPR2[j] = ROC_compute_3(yeta2)\n",
    "\n",
    "    px.line(x=temp_TPR2,y=temp_FPR2,title=str(i)+'.'+' ROC Curve of Variable '+str(index)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "As we can see, the performances of the models using all predictors are similar to the corresponding models using only the response-correlated predictors. That indicates our graphical lasso method is making sense. In conclusion, we can gain models with simple structures and little information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Choose your own adventure\n",
    "\n",
    "Using your own dataset (the high-dimensional dataset from Homework 1), perform penalized regression or penalized logistic regression and discuss your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('data.csv')\n",
    "df2=pd.read_csv('labels.csv')\n",
    "df1=df1.iloc[:,1:20532]\n",
    "df2=df2.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'data.csv' collects the gene expressions. 'labels.csv' represents the kinds of cancer.  \n",
    "To carry out a logistic regression, we only consider whether the label is 'LUAD' or not, making it a binary-classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.zeros(801)\n",
    "labels[df2=='LUAD']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import l1_min_c\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cs = l1_min_c(X_train, y_train, loss=\"log\") * np.logspace(0, 7, 16)\n",
    "\n",
    "clf = linear_model.LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"liblinear\",\n",
    "    tol=1e-6,\n",
    "    max_iter=int(1e6),\n",
    "    warm_start=True,\n",
    "    intercept_scaling=10000.0,\n",
    ")\n",
    "\n",
    "coefs_ = []\n",
    "error=[]\n",
    "for c in cs:\n",
    "    clf.set_params(C=c)\n",
    "    clf.fit(X_train, y_train)\n",
    "    coefs_.append(clf.coef_.ravel().copy())\n",
    "    error.append(np.mean(clf.predict(X_test)-y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms=[]\n",
    "for array in coefs_:\n",
    "    norms.append(np.sum(array!=0))\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "temp_model = LogisticRegression(fit_intercept=False,solver='liblinear').fit(X_train, y_train) \n",
    "np.mean((temp_model.predict(X_test)-y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(temp_model.coef_.ravel()!=0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
